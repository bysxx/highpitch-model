{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2Processor\n",
    "import torch\n",
    "model_id = 'hongseongpil/Wav2Vec2.0_zeroth_Ko'\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id,output_attentions=True)\n",
    "model.eval()\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pydub.effects import normalize\n",
    "from pydub import AudioSegment \n",
    "from pydub.silence import detect_nonsilent\n",
    "import numpy as np\n",
    "def getAudioInput(source_path , silentremove = True):\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(source_path).set_frame_rate(16000).set_sample_width(2).set_channels(1)\n",
    "        print(audio.duration_seconds)\n",
    "        newaudio = AudioSegment.empty()\n",
    "        if(silentremove):\n",
    "            result = detect_nonsilent(audio,min_silence_len=200,silence_thresh=-60)\n",
    "            newaudio = AudioSegment.empty()\n",
    "            for index in result:\n",
    "                newaudio += audio[index[0]:index[1]]\n",
    "            newaudio = normalize(newaudio)\n",
    "        else:\n",
    "            newaudio = normalize(audio)\n",
    "        return newaudio\n",
    "    except Exception as e:\n",
    "        print(\"오류 발생:\", e)\n",
    "def PrintAudioInfo(audio):\n",
    "    channels = audio.channels\n",
    "    sample_rate = audio.frame_rate\n",
    "    print(\"Channels:\", channels)\n",
    "    print(\"Sample rate:\", sample_rate)\n",
    "    print(\"Duration: \", audio.duration_seconds)\n",
    "    print(\"Bit depth:\", audio.sample_width, \"bits\") \n",
    "    print(\"len samples:\", len(np.array(audio.get_array_of_samples())))\n",
    "    display(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetLogits(audio):\n",
    "    input = processor(np.array(audio.get_array_of_samples(),dtype=np.float32), sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "    with torch.no_grad():\n",
    "        input_values = torch.tensor(input).unsqueeze(0)\n",
    "        return model(input_values).logits\n",
    "    \n",
    "def DisplayPred(logits , k):\n",
    "    sorted_tensor, indices = torch.sort(logits, dim=2, descending=True)\n",
    "    outputs = list()\n",
    "    for i,v in enumerate(indices[0]):\n",
    "        outputs.append((processor.tokenizer.convert_ids_to_tokens(v[:k].tolist()),v[:k].tolist()))\n",
    "    for i,v in enumerate(outputs):\n",
    "        print(sorted_tensor[0][i][:k],v)\n",
    "\n",
    "def DisplayResult(logits, audio , showaudio = False):\n",
    "    predlogits = torch.argmax(logits, dim=-1)[0]\n",
    "    outputs = processor.decode(predlogits,output_char_offsets=True)\n",
    "    time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate\n",
    "    PrintAudioInfo(audio)\n",
    "    print( [w['char'] for w in outputs['char_offsets']])\n",
    "    for w in outputs['char_offsets']:\n",
    "        start_offset = round (w[ \"start_offset\" ] * time_offset, 2 )\n",
    "        end_offset = round (w[ \"end_offset\" ] * time_offset, 2 )\n",
    "        print('word : ',w['char'],'start_offset :',start_offset,'end_offset :',end_offset)\n",
    "        if(showaudio):\n",
    "            display(audio[start_offset*1000:end_offset*1000])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오류 발생: [Errno 2] No such file or directory: 'weather_00207003.wav'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_array_of_samples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mTokenize_Kor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decompose_tokens\n\u001b[0;32m      2\u001b[0m audio \u001b[38;5;241m=\u001b[39m getAudioInput(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather_00207003.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m , \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 3\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mGetLogits\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m predlogits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mdecode(predlogits,output_char_offsets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mGetLogits\u001b[1;34m(audio)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGetLogits\u001b[39m(audio):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m processor(np\u001b[38;5;241m.\u001b[39marray(\u001b[43maudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_array_of_samples\u001b[49m(),dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32), sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_values[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      4\u001b[0m         input_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28minput\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_array_of_samples'"
     ]
    }
   ],
   "source": [
    "from Tokenize_Kor import decompose_tokens\n",
    "audio = getAudioInput(\"wav\" , False)\n",
    "logits = GetLogits(audio)\n",
    "predlogits = torch.argmax(logits, dim=-1)[0]\n",
    "outputs = processor.decode(predlogits,output_char_offsets=True)\n",
    "origintext = \"어제 친구랑 싸워서 기분이 안좋아..\"\n",
    "origintext = \"\".join(decompose_tokens(origintext)).replace(\" \",\"\")\n",
    "predtext = outputs['text'].replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅊㅣㄴㄱㅜㄹㅏㅇ\n",
      "ㅊㅣㄴㄱㅜㄹㅎㅏㅇ\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Tokenize_Kor import isVowel\n",
    "def levenshtein_distance(s1, s2):\n",
    "    m, n = len(s1), len(s2)\n",
    "    # DP 테이블 초기화\n",
    "    dp = [[0] * (n+1) for _ in range(m+1)]\n",
    "    # DP 테이블 첫 번째 행과 열 초기화\n",
    "    for i in range(1, m+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(1, n+1):\n",
    "        dp[0][j] = j\n",
    "    # DP 테이블 채우기\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                penalty = 1\n",
    "                s1_vowel = isVowel(s1[i-1])\n",
    "                s2_vowel = isVowel(s2[j-1])\n",
    "                if(s1_vowel == s2_vowel):\n",
    "                    penalty = 0.5\n",
    "\n",
    "                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + penalty\n",
    "    return dp[m][n], dp\n",
    "\n",
    "\n",
    "origintext = 'ㅊㅣㄴㄱㅜㄹㅏㅇ'\n",
    "predtext ='ㅊㅣㄴㄱㅜㄹㅎㅏㅇ'\n",
    "print(origintext)\n",
    "print(predtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "레벤슈타인 거리를 나타내는 matric을 구한다.<br>\n",
    "그리고 열과 행에서 유일하게 최솟값을 가질 때, 해당 matric에서 페널티가 적은 경로로 음소를 매칭한다.<br>\n",
    "멅티스레드 가능성...?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레벤슈타인 거리: 0.5\n",
      "DP 매트릭스:\n",
      "[[0.  1.  2.  3.  4.  5.  6.  7.  8.  9. ]\n",
      " [1.  0.  1.  1.5 2.  3.  3.5 4.  5.  5.5]\n",
      " [2.  1.  0.  1.  2.  2.5 3.5 4.5 4.5 5.5]\n",
      " [3.  1.5 1.  0.  0.5 1.5 2.  2.5 3.5 4. ]\n",
      " [4.  2.  2.  0.5 0.  1.  1.5 2.  3.  3.5]\n",
      " [5.  3.  2.5 1.5 1.  0.  1.  2.  2.5 3.5]\n",
      " [6.  3.5 3.5 2.  1.5 1.  0.  0.5 1.5 2. ]\n",
      " [7.  4.5 4.  3.  2.5 1.5 1.  1.  0.5 1.5]\n",
      " [8.  5.  5.  3.5 3.  2.5 1.5 1.5 1.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "distance, dp = levenshtein_distance(origintext, predtext)\n",
    "print(f\"레벤슈타인 거리: {distance}\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "dp_array = np.array(dp)\n",
    "print(\"DP 매트릭스:\")\n",
    "print(dp_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

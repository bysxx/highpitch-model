{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hongseongpil/wav2vec2-vocals were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hongseongpil/wav2vec2-vocals and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\zx288\\AppData\\Local\\Temp\\ipykernel_18232\\3144142929.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = torch.tensor(input).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "current_dir = os.getcwd()\n",
    "sys.path.append(os.path.dirname(os.path.abspath(current_dir)))\n",
    "from utils.Audio import PrintAudioInfo, GetAudio, remove_Silence\n",
    "from utils.levenshtein_distance import infer\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "from transformers import Wav2Vec2Processor\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_id = 'hongseongpil/wav2vec2-vocals'\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_id,output_attentions=True)\n",
    "model.eval()\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "from utils.Tokenize_Kor import decompose_tokens\n",
    "audio = remove_Silence(GetAudio(\"곰세마리.wav\")[:1000*20])\n",
    "input = processor(np.array(audio.set_channels(1).get_array_of_samples(),dtype=np.float32), sampling_rate=16000, return_tensors=\"pt\").input_values[0]\n",
    "with torch.no_grad():\n",
    "    input_values = torch.tensor(input).unsqueeze(0)\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "predlogits = torch.argmax(logits, dim=-1)[0]\n",
    "outputs = processor.decode(predlogits,output_char_offsets=True)\n",
    "\n",
    "origintext = \"곰 세마리가 한집에 있어 아빠곰 엄마곰 애기곰 아빠곰은 뚱뚱해 엄마 곰\"\n",
    "origintext = \"\".join(decompose_tokens(origintext)).replace(\" \",\"\")\n",
    "predtext = outputs['text'].replace(\" \",\"\")\n",
    "\n",
    "predlogits = torch.argmax(logits, dim=-1)[0]\n",
    "outputs = processor.decode(predlogits,output_char_offsets=True)\n",
    "time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infered = infer(origintext,predtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origintext: ㄱㅗㅁㅅㅔㅁㅏㄹㅣㄱㅏㅎㅏㄴㅈㅣㅂㅔㅇㅣㅆㅓㅇㅏㅃㅏㄱㅗㅁㅇㅓㅁㅁㅏㄱㅗㅁㅇㅐㄱㅣㄱㅗㅁㅇㅏㅃㅏㄱㅗㅁㅡㄴㄸㅜㅇㄸㅜㅇㅎㅐㅇㅓㅁㅁㅏㄱㅗㅁ\n",
      "predtext: ㄱㅓㅇㅓㄴㅅㅔㅇㅁㅏㅇㅏㄹㅣㅇㅣㄱㅏㅇㅏㅎㅏㅇㅏㅇㅈㅣㅂㅔㅇㅣㅆㅓㅇㅏㅇㅏㅃㅏㄱㅗㅇㅗㅁㅇㅓㅁㅏㄱㅓㅇㅗㅁㅇㅐㅂㅎㄹㅇㅣㄱㅗㅇㅗㅇㅏㅃㅏㅇㅏㄱㅗㅇㅗㅁㅡㅇㅡㄴㄸㅜㅇㅜㅁㅌㅜㅇㅜㅁㅎㅐㅇㅐㅇㅓㅁㅁㅏㅇㅏㄱㅓㅇㅓㅓ\n",
      "Matching....\n",
      "\n",
      "origintext: ㄱ predtext: ㄱ\n",
      "origintext: ㅗ predtext: ㅓㅇㅓ\n",
      "origintext: ㅁ predtext: ㄴ\n",
      "origintext: ㅅ predtext: ㅅ\n",
      "origintext: ㅔ predtext: ㅔㅇ\n",
      "origintext: ㅁ predtext: ㅁ\n",
      "origintext: ㅏ predtext: ㅏㅇㅏ\n",
      "origintext: ㄹ predtext: ㄹ\n",
      "origintext: ㅣ predtext: ㅣㅇㅣ\n",
      "origintext: ㄱ predtext: ㄱ\n",
      "origintext: ㅏ predtext: ㅏㅇㅏ\n",
      "origintext: ㅎ predtext: ㅎ\n",
      "origintext: ㅏ predtext: ㅏㅇㅏ\n",
      "origintext: ㄴ predtext: ㅇ\n",
      "origintext: ㅈ predtext: ㅈ\n",
      "origintext: ㅣ predtext: ㅣ\n",
      "origintext: ㅂ predtext: ㅂ\n",
      "origintext: ㅔ predtext: ㅔ\n",
      "origintext: ㅇ predtext: ㅇ\n",
      "origintext: ㅣ predtext: ㅣ\n",
      "origintext: ㅆ predtext: ㅆ\n",
      "origintext: ㅓ predtext: ㅓ\n",
      "origintext: ㅇ predtext: ㅇㅏㅇ\n",
      "origintext: ㅏ predtext: ㅏ\n",
      "origintext: ㅃ predtext: ㅃ\n",
      "origintext: ㅏ predtext: ㅏ\n",
      "origintext: ㄱ predtext: ㄱ\n",
      "origintext: ㅗ predtext: ㅗㅇㅗ\n",
      "origintext: ㅁ predtext: ㅁ\n",
      "origintext: ㅇ predtext: ㅇ\n",
      "origintext: ㅓ predtext: ㅓ\n",
      "origintext: ㅁ predtext: ㅁ\n",
      "origintext: ㅁ predtext: ㅁ\n",
      "origintext: ㅏ predtext: ㅏ\n",
      "origintext: ㄱ predtext: ㄱ\n",
      "origintext: ㅗ predtext: ㅓㅇㅗ\n",
      "origintext: ㅁ predtext: ㅁ\n",
      "origintext: ㅇ predtext: ㅇ\n",
      "origintext: ㅐ predtext: ㅐ\n",
      "origintext: ㄱ predtext: ㅂㅎㄹ\n",
      "origintext: ㅣ predtext: ㅇㅣ\n",
      "origintext: ㄱ predtext: ㄱ\n",
      "origintext: ㅗ predtext: ㅗㅇㅗ\n",
      "origintext: ㅁ predtext: ㅇ\n",
      "origintext: ㅇ predtext: ㅇ\n",
      "origintext: ㅏ predtext: ㅏ\n",
      "origintext: ㅃ predtext: ㅃ\n",
      "origintext: ㅏ predtext: ㅏㅇㅏ\n",
      "origintext: ㄱ predtext: ㄱ\n",
      "origintext: ㅗ predtext: ㅗㅇㅗ\n",
      "origintext: ㅁ predtext: ㅁ\n",
      "origintext: ㅡ predtext: ㅡㅇㅡ\n",
      "origintext: ㄴ predtext: ㄴ\n",
      "origintext: ㄸ predtext: ㄸ\n",
      "origintext: ㅜ predtext: ㅜㅇㅜ\n",
      "origintext: ㅇ predtext: ㅁ\n",
      "origintext: ㄸ predtext: ㅌ\n",
      "origintext: ㅜ predtext: ㅜㅇㅜ\n",
      "origintext: ㅇ predtext: ㅁ\n",
      "origintext: ㅎ predtext: ㅎ\n",
      "origintext: ㅐ predtext: ㅐㅇㅐ\n",
      "origintext: ㅇ predtext: ㅇ\n",
      "origintext: ㅓ predtext: ㅓ\n",
      "origintext: ㅁ predtext: ㅁㅁ\n",
      "origintext: ㅁ predtext: ㅁ\n",
      "origintext: ㅏ predtext: ㅏㅇㅏ\n",
      "origintext: ㄱ predtext: ㄱ\n",
      "origintext: ㅗ predtext: ㅓㅇㅓ\n",
      "origintext: ㅁ predtext: ㅓ\n",
      "word :  ㄱ start_offset : 7.26 end_offset : 7.28\n",
      "word :  ㅓ start_offset : 7.3 end_offset : 7.32\n",
      "word :  ㅇ start_offset : 7.32 end_offset : 7.36\n",
      "word :  ㅓ start_offset : 7.42 end_offset : 7.46\n",
      "word :  ㄴ start_offset : 7.52 end_offset : 7.54\n",
      "word :  ㅅ start_offset : 7.68 end_offset : 7.7\n",
      "word :  ㅔ start_offset : 7.74 end_offset : 7.76\n",
      "word :  ㅇ start_offset : 7.76 end_offset : 7.78\n",
      "word :  ㅁ start_offset : 7.94 end_offset : 7.96\n",
      "word :  ㅏ start_offset : 8.0 end_offset : 8.04\n",
      "word :  ㅇ start_offset : 8.04 end_offset : 8.06\n",
      "word :  ㅏ start_offset : 8.06 end_offset : 8.12\n",
      "word :  ㄹ start_offset : 8.18 end_offset : 8.22\n",
      "word :  ㅣ start_offset : 8.26 end_offset : 8.28\n",
      "word :  ㅇ start_offset : 8.28 end_offset : 8.32\n",
      "word :  ㅣ start_offset : 8.48 end_offset : 8.52\n",
      "word :  ㄱ start_offset : 8.64 end_offset : 8.66\n",
      "word :  ㅏ start_offset : 8.68 end_offset : 8.7\n",
      "word :  ㅇ start_offset : 8.7 end_offset : 8.74\n",
      "word :  ㅏ start_offset : 8.9 end_offset : 8.94\n",
      "word :  ㅎ start_offset : 9.0 end_offset : 9.04\n",
      "word :  ㅏ start_offset : 9.08 end_offset : 9.1\n",
      "word :  ㅇ start_offset : 9.1 end_offset : 9.14\n",
      "word :  ㅏ start_offset : 9.22 end_offset : 9.26\n",
      "word :  ㅇ start_offset : 9.34 end_offset : 9.36\n",
      "word :  ㅈ start_offset : 9.52 end_offset : 9.54\n",
      "word :  ㅣ start_offset : 9.56 end_offset : 9.6\n",
      "word :  ㅂ start_offset : 9.74 end_offset : 9.76\n",
      "word :  ㅔ start_offset : 9.8 end_offset : 9.82\n",
      "word :  ㅇ start_offset : 9.82 end_offset : 9.86\n",
      "word :  ㅣ start_offset : 10.0 end_offset : 10.14\n",
      "word :  ㅆ start_offset : 10.42 end_offset : 10.44\n",
      "word :  ㅓ start_offset : 10.48 end_offset : 10.5\n",
      "word :  ㅇ start_offset : 10.5 end_offset : 10.52\n",
      "word :  ㅏ start_offset : 10.62 end_offset : 10.64\n",
      "word :  ㅇ start_offset : 10.86 end_offset : 10.88\n",
      "word :  ㅏ start_offset : 10.88 end_offset : 10.94\n",
      "word :  ㅃ start_offset : 11.08 end_offset : 11.1\n",
      "word :  ㅏ start_offset : 11.14 end_offset : 11.2\n",
      "word :  ㄱ start_offset : 11.34 end_offset : 11.36\n",
      "word :  ㅗ start_offset : 11.4 end_offset : 11.42\n",
      "word :  ㅇ start_offset : 11.42 end_offset : 11.46\n",
      "word :  ㅗ start_offset : 11.5 end_offset : 11.52\n",
      "word :  ㅁ start_offset : 11.58 end_offset : 11.6\n",
      "word :  ㅇ start_offset : 11.8 end_offset : 11.82\n",
      "word :  ㅓ start_offset : 11.82 end_offset : 11.9\n",
      "word :  ㅁ start_offset : 12.0 end_offset : 12.02\n",
      "word :  ㅏ start_offset : 12.06 end_offset : 12.12\n",
      "word :  ㄱ start_offset : 12.24 end_offset : 12.26\n",
      "word :  ㅓ start_offset : 12.3 end_offset : 12.32\n",
      "word :  ㅇ start_offset : 12.32 end_offset : 12.36\n",
      "word :  ㅗ start_offset : 12.42 end_offset : 12.44\n",
      "word :  ㅁ start_offset : 12.48 end_offset : 12.5\n",
      "word :  ㅇ start_offset : 12.72 end_offset : 12.8\n",
      "word :  ㅐ start_offset : 13.02 end_offset : 13.04\n",
      "word :  ㅂ start_offset : 13.18 end_offset : 13.2\n",
      "word :  ㅎ start_offset : 13.2 end_offset : 13.22\n",
      "word :  ㄹ start_offset : 13.24 end_offset : 13.26\n",
      "word :  ㅇ start_offset : 13.26 end_offset : 13.3\n",
      "word :  ㅣ start_offset : 13.46 end_offset : 13.5\n",
      "word :  ㄱ start_offset : 13.62 end_offset : 13.64\n",
      "word :  ㅗ start_offset : 13.66 end_offset : 13.68\n",
      "word :  ㅇ start_offset : 13.68 end_offset : 13.72\n",
      "word :  ㅗ start_offset : 14.0 end_offset : 14.04\n",
      "word :  ㅇ start_offset : 14.5 end_offset : 14.6\n",
      "word :  ㅏ start_offset : 14.6 end_offset : 14.68\n",
      "word :  ㅃ start_offset : 14.96 end_offset : 14.98\n",
      "word :  ㅏ start_offset : 15.02 end_offset : 15.04\n",
      "word :  ㅇ start_offset : 15.04 end_offset : 15.08\n",
      "word :  ㅏ start_offset : 15.24 end_offset : 15.3\n",
      "word :  ㄱ start_offset : 15.42 end_offset : 15.44\n",
      "word :  ㅗ start_offset : 15.46 end_offset : 15.5\n",
      "word :  ㅇ start_offset : 15.5 end_offset : 15.54\n",
      "word :  ㅗ start_offset : 15.76 end_offset : 15.8\n",
      "word :  ㅁ start_offset : 15.88 end_offset : 15.9\n",
      "word :  ㅡ start_offset : 15.96 end_offset : 16.0\n",
      "word :  ㅇ start_offset : 16.0 end_offset : 16.04\n",
      "word :  ㅡ start_offset : 16.04 end_offset : 16.08\n",
      "word :  ㄴ start_offset : 16.12 end_offset : 16.16\n",
      "word :  ㄸ start_offset : 16.34 end_offset : 16.36\n",
      "word :  ㅜ start_offset : 16.4 end_offset : 16.42\n",
      "word :  ㅇ start_offset : 16.42 end_offset : 16.46\n",
      "word :  ㅜ start_offset : 16.46 end_offset : 16.54\n",
      "word :  ㅁ start_offset : 16.6 end_offset : 16.62\n",
      "word :  ㅌ start_offset : 16.82 end_offset : 16.84\n",
      "word :  ㅜ start_offset : 16.88 end_offset : 16.9\n",
      "word :  ㅇ start_offset : 16.9 end_offset : 16.94\n",
      "word :  ㅜ start_offset : 17.02 end_offset : 17.04\n",
      "word :  ㅁ start_offset : 17.12 end_offset : 17.14\n",
      "word :  ㅎ start_offset : 17.2 end_offset : 17.24\n",
      "word :  ㅐ start_offset : 17.28 end_offset : 17.3\n",
      "word :  ㅇ start_offset : 17.3 end_offset : 17.34\n",
      "word :  ㅐ start_offset : 17.66 end_offset : 17.68\n",
      "word :  ㅇ start_offset : 18.14 end_offset : 18.24\n",
      "word :  ㅓ start_offset : 18.24 end_offset : 18.36\n",
      "word :  ㅁ start_offset : 18.4 end_offset : 18.42\n",
      "word :  ㅁ start_offset : 18.58 end_offset : 18.6\n",
      "word :  ㅏ start_offset : 18.64 end_offset : 18.68\n",
      "word :  ㅇ start_offset : 18.68 end_offset : 18.72\n",
      "word :  ㅏ start_offset : 18.88 end_offset : 18.92\n",
      "word :  ㄱ start_offset : 19.06 end_offset : 19.08\n",
      "word :  ㅓ start_offset : 19.12 end_offset : 19.14\n",
      "word :  ㅇ start_offset : 19.14 end_offset : 19.18\n",
      "word :  ㅓ start_offset : 19.36 end_offset : 19.42\n",
      "word :  ㅓ start_offset : 19.5 end_offset : 19.52\n"
     ]
    }
   ],
   "source": [
    "print(\"origintext:\", origintext)\n",
    "print(\"predtext:\",predtext)\n",
    "\n",
    "print(\"Matching....\\n\")\n",
    "\n",
    "for item in infered:\n",
    "    pred =\"\".join([predtext[j-1] for j in item[1]])\n",
    "    print(\"origintext:\", origintext[item[0]-1],\"predtext:\",pred)\n",
    "\n",
    "for w in outputs['char_offsets']:\n",
    "    start_offset = round (w[ \"start_offset\" ] * time_offset, 2 )\n",
    "    end_offset = round (w[ \"end_offset\" ] * time_offset, 2 )\n",
    "    print('word : ',w['char'],'start_offset :',start_offset,'end_offset :',end_offset)\n",
    "    # display(audio[start_offset*1000:end_offset*1000])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
